{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pyreadr\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "# check pytorch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class to get our dataset.\n",
    "class Data(Dataset):\n",
    "    def __init__(self, Dataset, x, y):\n",
    "        self.x=torch.FloatTensor(Dataset[x].values)\n",
    "        self.y=torch.FloatTensor(Dataset[y].values)\n",
    "        self.y=self.y.view(-1,1)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):         \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df,pred_vars, calculate_parameters = False ): #only normalizes the pred_vars and eonr\n",
    "    result = df.copy()\n",
    "    if calculate_parameters:\n",
    "        frame = { 'mean': df.mean(), 'std': df.std()} \n",
    "        parameters = pd.DataFrame(frame) \n",
    "\n",
    "        parameters.reset_index(inplace=True) # Resets the index, makes factor a column  \n",
    "        parameters = parameters[parameters['index'].isin(pred_vars+['eonr'])]\n",
    "        parameters.to_pickle(\"/home/germanm2/n_policy_box/Data/files_rds/normalization_parameters.pkl\")\n",
    "    else:\n",
    "        parameters = pd.read_pickle(\"/home/germanm2/n_policy_box/Data/files_rds/normalization_parameters.pkl\")\n",
    "    parameters_names = parameters['index']\n",
    "    parameters_names_both = parameters_names[parameters_names.isin(list(df))]    \n",
    "    for feature_name in parameters_names_both:\n",
    "        #print(feature_name)\n",
    "        mean_value = parameters.loc[(parameters['index'] == feature_name), 'mean']\n",
    "        std_value = parameters.loc[(parameters['index'] == feature_name), 'std']\n",
    "        result[feature_name] = (df[feature_name] - mean_value.values)/std_value.values\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our training dataset.\n",
    "pred_vars = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/pred_vars.rds\")[None] # also works for RData\n",
    "pred_vars = [item for sublist in pred_vars.values.tolist()  for item in sublist]\n",
    "\n",
    "TrainSet_eonr2_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/TrainSet_eonr2.rds\")[None] # also works for RData\n",
    "training_set = normalize(TrainSet_eonr2_df, pred_vars, calculate_parameters = True)\n",
    "training_set2=Data(training_set, x = pred_vars, y = 'eonr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our validation dataset\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "prediction_set_aggregated_df = prediction_set_aggregated_df.rename(columns={\"eonr_12\": \"eonr\"}) #needed for tha Date class\n",
    "validation_set = normalize(prediction_set_aggregated_df, pred_vars, calculate_parameters = False)\n",
    "validation_set2 = Data(validation_set, x=pred_vars, y='eonr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(prediction_set_aggregated_df['eonr'], validation_set['eonr'], 'o', color='black')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_back(df): #only normalizes_back the pred_vars and eonr available at df\n",
    "    parameters = pd.read_pickle(\"/home/germanm2/n_policy_box/Data/files_rds/normalization_parameters.pkl\")\n",
    "    result = df.copy()\n",
    "    parameters_names = parameters['index']\n",
    "    parameters_names_both = parameters_names[parameters_names.isin(list(df))]    \n",
    "    for feature_name in parameters_names_both:\n",
    "        mean_value = parameters.loc[(parameters['index'] == feature_name), 'mean']\n",
    "        std_value = parameters.loc[(parameters['index'] == feature_name), 'std']\n",
    "        result[feature_name] = (df[feature_name]  * std_value.values) + mean_value.values\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the class for model \n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, cols):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.hidden1 = torch.nn.Linear(cols, 21)   # hidden layer\n",
    "#         self.hidden2 = torch.nn.Linear(21, 10)   # hidden layer\n",
    "#         self.predict = torch.nn.Linear(10, 1)   # output layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.hidden1(x))      # activation function for hidden layer\n",
    "#         x = F.relu(self.hidden2(x))      # activation function for hidden layer\n",
    "#         x = self.predict(x)             # linear output\n",
    "#         return x\n",
    "    \n",
    "# cols = len(pred_vars)\n",
    "# model = Net(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class for model (with Dropout)\n",
    "class Net(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, cols, p=0.1):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p=p)\n",
    "        self.hidden1 = torch.nn.Linear(cols, 30)   # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(30, 5)   # hidden layer\n",
    "        self.hidden3 = torch.nn.Linear(5, 5)   # hidden layer\n",
    "        self.hidden4 = torch.nn.Linear(5, 5)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(5, 1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop(self.hidden1(x)))      # activation function for hidden layer\n",
    "        x = F.relu(self.drop(self.hidden2(x)))      # activation function for hidden layer\n",
    "        x = F.relu(self.drop(self.hidden3(x)))      # activation function for hidden layer\n",
    "        x = F.relu(self.drop(self.hidden4(x)))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the class for model (simple)\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, cols):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.hidden1 = torch.nn.Linear(cols, 1)   # hidden layer\n",
    "#         self.predict = torch.nn.Linear(1, 1)   # output layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.hidden1(x))      # activation function for hidden layer\n",
    "#         x = self.predict(x)             # linear output\n",
    "#         return x\n",
    "    \n",
    "# cols = len(pred_vars)\n",
    "# model = Net(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make one prediction\n",
    "# x = training_set[0][0]\n",
    "# y = training_set[0][0]\n",
    "# print(model.state_dict())\n",
    "# print(model(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the function to train our model, which accumulate lost for each iteration to obtain the cost.\n",
    "def train_mini_batches(train_loader, validation_set2, model,criterion, optimizer, epochs=5, plot_epoch = False):\n",
    "    cost_training=[] \n",
    "    cost_validation=[]\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            yhat=model(x)\n",
    "            loss=criterion(yhat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total+=loss.item()\n",
    "        cost_training.append(total)\n",
    "        cost_validation.append(criterion(model(validation_set2.x),validation_set2.y))\n",
    "\n",
    "    if plot_epoch:\n",
    "        fig, ax1 = plt.subplots()\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('epoch)')\n",
    "        ax1.set_ylabel('cost', color=color)\n",
    "        ax1.plot(cost_training, color=color, label='training') \n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('cost', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(cost_validation, color=color, label='validation') \n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        fig.legend()\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_set2, validation_set2, model,criterion, optimizer, learning_rate, epochs=5, plot_epoch = False):\n",
    "    cost_training=[] \n",
    "    cost_validation=[]  \n",
    "    for epoch in range(epochs):\n",
    "         #all the samples are used for training because they fit in memory\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(training_set2.x)\n",
    "        loss=criterion(yhat,training_set2.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cost_training.append(loss)\n",
    "        cost_validation.append(criterion(model(validation_set2.x),validation_set2.y))\n",
    "#         if epoch > 500: #this will lower the learning rate at the end of the training\n",
    "#             optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate/10)\n",
    "    if plot_epoch:\n",
    "        fig, ax1 = plt.subplots()\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('epoch)')\n",
    "        ax1.set_ylabel('cost', color=color)\n",
    "        ax1.plot(cost_training, color=color, label='training') \n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('cost', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(cost_validation, color=color, label='validation') \n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        fig.legend()\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.show()\n",
    "#     plt.figure()\n",
    "#     plt.plot(cost_training, label='training') \n",
    "#     plt.plot(cost_validation, label='validation') \n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a single item from enumerate\n",
    "\n",
    "# singleitem = next(iter(train_loader))\n",
    "# x = singleitem[0]\n",
    "# y = singleitem[1]\n",
    "# yhat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model \n",
    "cols = len(pred_vars)\n",
    "model = Net(cols, p=0.3)\n",
    "model.train()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "learning_rate = 0.01\n",
    "criterion = torch.nn.MSELoss(reduction='mean')  # this is for regression mean squared loss \n",
    "# criterion = torch.nn.L1Loss() # this is for regression mean absolute loss \n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(training_set2, validation_set2, model,criterion, optimizer, learning_rate, epochs=1000, plot_epoch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with mini batches\n",
    "# model_mini = Net(cols, p=0.3)\n",
    "# model_mini.train()\n",
    "# train_loader=DataLoader(dataset=training_set2,batch_size=32, shuffle=True)\n",
    "# train_mini_batches(train_loader, validation_set2, model_mini, criterion, optimizer, epochs=1000, plot_epoch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our prediction dataset (here is the same than validation, but shouldn't be)\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RDat\n",
    "\n",
    "prediction_set = normalize(prediction_set_aggregated_df, pred_vars, calculate_parameters = False)\n",
    "prediction_set_tensor = Variable(torch.FloatTensor(prediction_set[pred_vars].values)) #we don't use the data class, because there is no eonr column\n",
    "model.eval()\n",
    "y_pred = model(prediction_set_tensor) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "\n",
    "prediction_set['eonr'] = y_pred #needed to have that name for the normalization function\n",
    "prediction_set2 = normalize_back(prediction_set)\n",
    "# prediction_set2 = prediction_set2.rename(columns={\"eonr\": \"eonr_pred_cnn\"}) \n",
    "\n",
    "prediction_set_aggregated_df['eonr_pred_cnn']= prediction_set2['eonr']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred_cnn'], 'o', color='black')\n",
    "plt.xlabel('eonr_12')\n",
    "plt.ylabel('eonr_pred_cnn')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred_rf'], 'o', color='black')\n",
    "plt.xlabel('eonr_12')\n",
    "plt.ylabel('eonr_pred_rf')\n",
    "plt.show()\n",
    "\n",
    "plot_df = prediction_set_aggregated_df.groupby('eonr_12', as_index=False)['eonr_pred_cnn','eonr_pred_rf'].mean()\n",
    "plt.figure()\n",
    "plt.plot(plot_df['eonr_12'], plot_df['eonr_pred_cnn'], label='cnn') \n",
    "plt.plot(plot_df['eonr_12'], plot_df['eonr_pred_rf'], label='rf') \n",
    "plt.xlabel('eonr_12')\n",
    "plt.ylabel('eonr_pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(prediction_set_aggregated_df['eonr_pred_cnn'].min(),\n",
    "prediction_set_aggregated_df['eonr_pred_cnn'].max(),\n",
    "prediction_set_aggregated_df['eonr_pred_cnn'].mean(),\n",
    "mean_squared_error(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred_cnn']),\n",
    "mean_squared_error(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred_rf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4050 3472 (21, 21, 10, 1)  3322 (21,10,5,1) 2983 (10, 5, 5, 5, 1) 2167 2164 3004 2001 1893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([100,160],dtype=torch.float32)\n",
    "yhat= torch.tensor([110,170],dtype=torch.float32)\n",
    "criterion1 = torch.nn.L1Loss()\n",
    "criterion2 = torch.nn.MSELoss(reduction='mean')  # this is for regression mean squared loss \n",
    "print(criterion1(y, yhat), criterion2(y, yhat))\n",
    "criterion_up(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_up(y, yhat):\n",
    "    errors = yhat - y\n",
    "    errors_subpred = errors[errors < 0]\n",
    "    errors_overpred = errors[errors > 0]\n",
    "    if errors_subpred.data.nelement()>0: #correction for zero length\n",
    "        criterion_subpred = sum((errors_subpred)**2) / errors_subpred.data.nelement()\n",
    "    else:\n",
    "        criterion_subpred = torch.tensor([0],dtype=torch.float32)\n",
    "    if errors_overpred.data.nelement()>0: #correction for zero length\n",
    "        criterion_overpred = sum((errors_overpred)) / errors_overpred.data.nelement()\n",
    "    else:\n",
    "        criterion_overpred = torch.tensor([0],dtype=torch.float32)\n",
    "    criterion = criterion_subpred.add(criterion_overpred) \n",
    "    criterion\n",
    "    return(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat = model(data_set.x)\n",
    "y = torch.tensor([100,160,170,130],dtype=torch.float32)\n",
    "yhat= torch.tensor([90,170,150,150],dtype=torch.float32)\n",
    "\n",
    "# loss=criterion_up(yhat,data_set.y)\n",
    "errors = yhat - y\n",
    "errors\n",
    "errors_subpred = errors[errors < 0]\n",
    "errors_overpred = errors[errors > 0]\n",
    "\n",
    "if errors_subpred.data.nelement()>0: #correction for zero length\n",
    "    criterion_subpred = sum((errors_subpred)**2) / errors_subpred.data.nelement()\n",
    "else:\n",
    "    criterion_subpred = torch.tensor([0],dtype=torch.float32)\n",
    "if errors_overpred.data.nelement()>0: #correction for zero length\n",
    "    criterion_overpred = sum((errors_overpred)) / errors_overpred.data.nelement()\n",
    "else:\n",
    "    criterion_overpred = torch.tensor([0],dtype=torch.float32)\n",
    "criterion = criterion_subpred.add(criterion_overpred) \n",
    "criterion\n",
    "yhat = model(data_set.x)\n",
    "criterion = criterion_up\n",
    "criterion(data_set.y,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(TrainSet_eonr2_df, policy, pred_vars):\n",
    "    training_set = normalize(TrainSet_eonr2_df, pred_vars, calculate_parameters = False)\n",
    "    training_set2=Data(training_set, x = pred_vars, y = 'eonr')\n",
    "    \n",
    "    cols = len(pred_vars)\n",
    "\n",
    "    # Create our model \n",
    "    model = Net(cols, p=0.3)\n",
    "    model.train()\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    learning_rate = 0.01\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')  # this is for regression mean squared loss \n",
    "    # criterion = torch.nn.L1Loss() # this is for regression mean absolute loss \n",
    "\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train(training_set2, validation_set2, model, criterion, optimizer, learning_rate, epochs=1000, plot_epoch = False)\n",
    "    path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "    torch.save(model.state_dict(), path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = 'ratio_5'\n",
    "pred_vars = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/pred_vars.rds\")[None] # also works for RData\n",
    "pred_vars = [item for sublist in pred_vars.values.tolist()  for item in sublist]\n",
    "TrainSet_eonr2_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/TrainSet_eonr2.rds\")[None] # also works for RData\n",
    "net_return = build_cnn(TrainSet_eonr2_df, policy, pred_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with returned model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X = Variable(torch.FloatTensor(prediction_set_aggregated_df[pred_vars].values)) \n",
    "\n",
    "y_pred = model(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with returned model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X_pred = prediction_set_aggregated_df[pred_vars]\n",
    "\n",
    "X_pred=X_pred.values\n",
    "\n",
    "X = Variable(torch.FloatTensor(X_pred)) \n",
    "y_pred = net_return(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "y_pred\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "\n",
    "# now let's write a Rds\n",
    "pyreadr.write_rds(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_cnn_dt.rds\", prediction_set_aggregated_df)\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved model\n",
    "policy = 'ratio_5'\n",
    "path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "net_load = Net(21, 100, 1)\n",
    "net_load.load_state_dict(torch.load(path))\n",
    "net_load.eval()\n",
    "net_load.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with the saved model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X_pred = prediction_set_aggregated_df[pred_vars]\n",
    "X_pred=X_pred.values\n",
    "X_pred\n",
    "X = Variable(torch.FloatTensor(X_pred)) \n",
    "y_pred = net_load(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "y_pred\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "\n",
    "# now let's write a Rds\n",
    "pyreadr.write_rds(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_cnn_dt.rds\", prediction_set_aggregated_df)\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that loads the saved model and does predictions\n",
    "\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None]\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn(prediction_set_aggregated_df, policy, pred_vars):\n",
    "    #Load the saved model\n",
    "    #policy = 'ratio_5'\n",
    "    cols = len(pred_vars)\n",
    "    model_load = Net(cols)\n",
    "    path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "    model_load.load_state_dict(torch.load(path))\n",
    "    model_load.eval()\n",
    "    model_load.state_dict()\n",
    "\n",
    "    prediction_set = normalize(prediction_set_aggregated_df, pred_vars, calculate_parameters = False)\n",
    "    prediction_set_tensor = Variable(torch.FloatTensor(prediction_set[pred_vars].values)) #we don't use the data class, because there is no eonr column\n",
    "    y_pred = model_load(prediction_set_tensor) #This outputs the value for regression\n",
    "    y_pred=y_pred.data[:,0].numpy()\n",
    "\n",
    "    prediction_set['eonr'] = y_pred #needed to have that name for the normalization function\n",
    "    prediction_set2 = normalize_back(prediction_set)\n",
    "    # prediction_set2 = prediction_set2.rename(columns={\"eonr\": \"eonr_pred_cnn\"}) \n",
    "\n",
    "    prediction_set_aggregated_df['eonr_pred']= prediction_set2['eonr']\n",
    "    return prediction_set_aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "prediction_set_aggregated_df2 = predict_cnn(prediction_set_aggregated_df, 'ratio_5', pred_vars)\n",
    "prediction_set_aggregated_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(prediction_set_aggregated_df['eonr_pred'].min(),\n",
    "prediction_set_aggregated_df['eonr_pred'].max(),\n",
    "prediction_set_aggregated_df['eonr_pred'].mean(),\n",
    "mean_squared_error(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3088\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "x = prediction_set_aggregated_df['eonr_12']\n",
    "y = prediction_set_aggregated_df['eonr_pred']\n",
    "\n",
    "plt.scatter(x, y, c=\"g\", alpha=0.5, marker=r'$\\clubsuit$',\n",
    "            label=\"Luck\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GEOANN]",
   "language": "python",
   "name": "conda-env-GEOANN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
