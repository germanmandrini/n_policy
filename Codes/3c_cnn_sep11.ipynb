{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pyreadr\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "# check pytorch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class to get our dataset.\n",
    "class Data(Dataset):\n",
    "    def __init__(self, Dataset, x, y):\n",
    "        self.x=torch.FloatTensor(Dataset[pred_vars].values)\n",
    "        self.y=torch.FloatTensor(Dataset[y].values)\n",
    "        self.y=self.y.view(-1,1)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):         \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, parameters = None):\n",
    "    result = df.copy()\n",
    "    if parameters is None:\n",
    "        frame = { 'mean': df.mean(), 'std': df.std()} \n",
    "        parameters = pd.DataFrame(frame) \n",
    "        parameters.reset_index(inplace=True) # Resets the index, makes factor a column   \n",
    "        \n",
    "    for feature_name in df.columns:\n",
    "        mean_value = parameters.loc[(parameters['index'] == feature_name), 'mean']\n",
    "        std_value = parameters.loc[(parameters['index'] == feature_name), 'std']\n",
    "        result[feature_name] = (df[feature_name] - mean_value.values)/std_value.values\n",
    "    return [result, parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our training dataset.\n",
    "pred_vars = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/pred_vars.rds\")[None] # also works for RData\n",
    "pred_vars = [item for sublist in pred_vars.values.tolist()  for item in sublist]\n",
    "\n",
    "TrainSet_eonr2_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/TrainSet_eonr2.rds\")[None] # also works for RData\n",
    "data_and_parameters = normalize(TrainSet_eonr2_df)\n",
    "data_set = data_and_parameters[0]\n",
    "parameters_training = data_and_parameters[1]\n",
    "data_set2=Data(data_set, x = pred_vars, y = 'eonr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our validation dataset.\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "prediction_set_aggregated_df = prediction_set_aggregated_df.rename(columns={\"eonr_12\": \"eonr\"}) #needed for tha Date class\n",
    "# validation_set = scaler.transform(prediction_set_aggregated_df[pred_vars+['eonr']])\n",
    "# validation_set2 = Data(validation_set, x=pred_vars, y='eonr')\n",
    "validation_set = normalize(TrainSet_eonr2_df, parameters = parameters_training)[0]\n",
    "validation_set\n",
    "validation_set2 = Data(validation_set, x=pred_vars, y='eonr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_back(df, parameters = None):\n",
    "    df = validation_set\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        mean_value = parameters.loc[(parameters['index'] == feature_name), 'mean']\n",
    "        std_value = parameters.loc[(parameters['index'] == feature_name), 'std']\n",
    "        result[feature_name] = (df[feature_name]  * std_value.values) + mean_value.values\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eonr</th>\n",
       "      <th>rain_30</th>\n",
       "      <th>rain_60</th>\n",
       "      <th>rain_90</th>\n",
       "      <th>t_max_30</th>\n",
       "      <th>t_max_60</th>\n",
       "      <th>t_max_90</th>\n",
       "      <th>t_min_30</th>\n",
       "      <th>t_min_60</th>\n",
       "      <th>t_min_90</th>\n",
       "      <th>...</th>\n",
       "      <th>day_sow</th>\n",
       "      <th>day_v5</th>\n",
       "      <th>lai_v5</th>\n",
       "      <th>whc</th>\n",
       "      <th>oc_20cm_v5</th>\n",
       "      <th>sw_dep_v5</th>\n",
       "      <th>n_0_60cm_v5</th>\n",
       "      <th>surfaceom_wt_v5</th>\n",
       "      <th>sand_40cm</th>\n",
       "      <th>clay_40cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>22.766667</td>\n",
       "      <td>15.550000</td>\n",
       "      <td>5.433333</td>\n",
       "      <td>10.383333</td>\n",
       "      <td>4.183333</td>\n",
       "      <td>-2.783333</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.528</td>\n",
       "      <td>288.0</td>\n",
       "      <td>2.03525</td>\n",
       "      <td>637.087</td>\n",
       "      <td>32.7</td>\n",
       "      <td>310.8</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.033333</td>\n",
       "      <td>15.050000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>9.783333</td>\n",
       "      <td>1.783333</td>\n",
       "      <td>-0.766667</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.570</td>\n",
       "      <td>288.0</td>\n",
       "      <td>2.03500</td>\n",
       "      <td>659.419</td>\n",
       "      <td>38.3</td>\n",
       "      <td>370.4</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>19.866667</td>\n",
       "      <td>18.083333</td>\n",
       "      <td>7.466667</td>\n",
       "      <td>8.033333</td>\n",
       "      <td>5.816667</td>\n",
       "      <td>-3.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.472</td>\n",
       "      <td>288.0</td>\n",
       "      <td>2.03525</td>\n",
       "      <td>728.974</td>\n",
       "      <td>40.3</td>\n",
       "      <td>318.9</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>18.850000</td>\n",
       "      <td>15.216667</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>7.816667</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>288.0</td>\n",
       "      <td>2.03575</td>\n",
       "      <td>647.879</td>\n",
       "      <td>30.5</td>\n",
       "      <td>330.7</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.183333</td>\n",
       "      <td>14.066667</td>\n",
       "      <td>7.183333</td>\n",
       "      <td>6.850000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>-3.183333</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.186</td>\n",
       "      <td>288.0</td>\n",
       "      <td>2.03350</td>\n",
       "      <td>624.983</td>\n",
       "      <td>57.0</td>\n",
       "      <td>610.6</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>120.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>23.866667</td>\n",
       "      <td>13.933333</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>11.216667</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>-1.616667</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>294.5</td>\n",
       "      <td>1.16825</td>\n",
       "      <td>659.938</td>\n",
       "      <td>9.8</td>\n",
       "      <td>258.2</td>\n",
       "      <td>7.655814</td>\n",
       "      <td>18.032558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>300.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>22.266667</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>10.033333</td>\n",
       "      <td>1.283333</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.526</td>\n",
       "      <td>294.5</td>\n",
       "      <td>1.16825</td>\n",
       "      <td>650.698</td>\n",
       "      <td>10.5</td>\n",
       "      <td>263.5</td>\n",
       "      <td>7.655814</td>\n",
       "      <td>18.032558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>210.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>21.966667</td>\n",
       "      <td>13.066667</td>\n",
       "      <td>15.350000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.330</td>\n",
       "      <td>294.5</td>\n",
       "      <td>1.16700</td>\n",
       "      <td>589.504</td>\n",
       "      <td>32.2</td>\n",
       "      <td>448.5</td>\n",
       "      <td>7.655814</td>\n",
       "      <td>18.032558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>180.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>22.733333</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>-1.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.543</td>\n",
       "      <td>294.5</td>\n",
       "      <td>1.16775</td>\n",
       "      <td>599.784</td>\n",
       "      <td>16.8</td>\n",
       "      <td>342.0</td>\n",
       "      <td>7.655814</td>\n",
       "      <td>18.032558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>230.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>11.616667</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>-1.150000</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.558</td>\n",
       "      <td>294.5</td>\n",
       "      <td>1.16800</td>\n",
       "      <td>647.014</td>\n",
       "      <td>8.2</td>\n",
       "      <td>268.3</td>\n",
       "      <td>7.655814</td>\n",
       "      <td>18.032558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4263 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       eonr  rain_30  rain_60  rain_90   t_max_30   t_max_60   t_max_90  \\\n",
       "0     210.0     94.0    191.0     90.0  22.766667  15.550000   5.433333   \n",
       "1     230.0    124.0    123.0     17.0  22.033333  15.050000  12.100000   \n",
       "2     240.0    271.0     98.0     46.0  19.866667  18.083333   7.466667   \n",
       "3     200.0    151.0     69.0    113.0  18.850000  15.216667   9.250000   \n",
       "4     140.0     18.0    112.0     32.0  22.183333  14.066667   7.183333   \n",
       "...     ...      ...      ...      ...        ...        ...        ...   \n",
       "4258  120.0    259.0    143.0     62.0  23.866667  13.933333  11.500000   \n",
       "4259  300.0    201.0    124.0     66.0  22.266667  13.500000  13.900000   \n",
       "4260  210.0     65.0    115.0     61.0  21.966667  13.066667  15.350000   \n",
       "4261  180.0     92.0    108.0     73.0  22.733333  16.000000  11.250000   \n",
       "4262  230.0    370.0     94.0    124.0  22.000000  12.666667  11.616667   \n",
       "\n",
       "       t_min_30  t_min_60  t_min_90  ...  day_sow  day_v5  lai_v5    whc  \\\n",
       "0     10.383333  4.183333 -2.783333  ...    111.0   147.0   0.528  288.0   \n",
       "1      9.783333  1.783333 -0.766667  ...    111.0   150.0   0.570  288.0   \n",
       "2      8.033333  5.816667 -3.550000  ...    111.0   157.0   0.472  288.0   \n",
       "3      7.816667  2.733333 -0.466667  ...    111.0   152.0   0.532  288.0   \n",
       "4      6.850000  3.250000 -3.183333  ...    111.0   154.0   0.186  288.0   \n",
       "...         ...       ...       ...  ...      ...     ...     ...    ...   \n",
       "4258  11.216667  1.916667 -1.616667  ...     95.0   128.0   0.537  294.5   \n",
       "4259  10.033333  1.283333  2.950000  ...     95.0   136.0   0.526  294.5   \n",
       "4260   9.100000  0.916667  3.500000  ...     95.0   131.0   0.330  294.5   \n",
       "4261   9.100000  2.750000 -1.350000  ...     95.0   133.0   0.543  294.5   \n",
       "4262  10.250000  0.966667 -1.150000  ...     95.0   137.0   0.558  294.5   \n",
       "\n",
       "      oc_20cm_v5  sw_dep_v5  n_0_60cm_v5  surfaceom_wt_v5  sand_40cm  \\\n",
       "0        2.03525    637.087         32.7            310.8   4.000000   \n",
       "1        2.03500    659.419         38.3            370.4   4.000000   \n",
       "2        2.03525    728.974         40.3            318.9   4.000000   \n",
       "3        2.03575    647.879         30.5            330.7   4.000000   \n",
       "4        2.03350    624.983         57.0            610.6   4.000000   \n",
       "...          ...        ...          ...              ...        ...   \n",
       "4258     1.16825    659.938          9.8            258.2   7.655814   \n",
       "4259     1.16825    650.698         10.5            263.5   7.655814   \n",
       "4260     1.16700    589.504         32.2            448.5   7.655814   \n",
       "4261     1.16775    599.784         16.8            342.0   7.655814   \n",
       "4262     1.16800    647.014          8.2            268.3   7.655814   \n",
       "\n",
       "      clay_40cm  \n",
       "0     24.262000  \n",
       "1     24.262000  \n",
       "2     24.262000  \n",
       "3     24.262000  \n",
       "4     24.262000  \n",
       "...         ...  \n",
       "4258  18.032558  \n",
       "4259  18.032558  \n",
       "4260  18.032558  \n",
       "4261  18.032558  \n",
       "4262  18.032558  \n",
       "\n",
       "[4263 rows x 21 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_back(validation_set, parameters = parameters_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the class for model \n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, cols):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.hidden1 = torch.nn.Linear(cols, 21)   # hidden layer\n",
    "#         self.hidden2 = torch.nn.Linear(21, 10)   # hidden layer\n",
    "#         self.predict = torch.nn.Linear(10, 1)   # output layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.hidden1(x))      # activation function for hidden layer\n",
    "#         x = F.relu(self.hidden2(x))      # activation function for hidden layer\n",
    "#         x = self.predict(x)             # linear output\n",
    "#         return x\n",
    "    \n",
    "# cols = len(pred_vars)\n",
    "# model = Net(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class for model (with Dropout)\n",
    "class Net(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, cols, p=0.1):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p=p)\n",
    "        self.hidden1 = torch.nn.Linear(cols, 30)   # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(30, 5)   # hidden layer\n",
    "        self.hidden3 = torch.nn.Linear(5, 5)   # hidden layer\n",
    "        self.hidden4 = torch.nn.Linear(5, 5)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(5, 1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop(self.hidden1(x)))      # activation function for hidden layer\n",
    "        x = F.relu(self.drop(self.hidden2(x)))      # activation function for hidden layer\n",
    "        x = F.relu(self.drop(self.hidden3(x)))      # activation function for hidden layer\n",
    "        x = F.relu(self.drop(self.hidden4(x)))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the class for model (simple)\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, cols):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.hidden1 = torch.nn.Linear(cols, 1)   # hidden layer\n",
    "#         self.predict = torch.nn.Linear(1, 1)   # output layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.hidden1(x))      # activation function for hidden layer\n",
    "#         x = self.predict(x)             # linear output\n",
    "#         return x\n",
    "    \n",
    "# cols = len(pred_vars)\n",
    "# model = Net(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make one prediction\n",
    "# x = training_set[0][0]\n",
    "# y = training_set[0][0]\n",
    "# print(model.state_dict())\n",
    "# print(model(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the function to train our model, which accumulate lost for each iteration to obtain the cost.\n",
    "def train_mini_batches(training_set, train_loader, validation_set, model,criterion, optimizer, epochs=5, plot_epoch = False):\n",
    "    cost_training=[] \n",
    "    cost_validation=[]\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            yhat=model(x)\n",
    "            loss=criterion(yhat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total+=loss.item()\n",
    "        cost_training.append(total)\n",
    "        cost_validation.append(criterion(model(validation_set.x),validation_set.y))\n",
    "\n",
    "    if plot_epoch:\n",
    "        fig, ax1 = plt.subplots()\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('epoch)')\n",
    "        ax1.set_ylabel('cost', color=color)\n",
    "        ax1.plot(cost_training, color=color, label='training') \n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('cost', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(cost_validation, color=color, label='validation') \n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        fig.legend()\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_set2, validation_set2, model,criterion, optimizer, learning_rate, epochs=5, plot_epoch = False):\n",
    "    cost_training=[] \n",
    "    cost_validation=[]  \n",
    "    for epoch in range(epochs):\n",
    "         #all the samples are used for training because they fit in memory\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(training_set2.x)\n",
    "        loss=criterion(yhat,training_set2.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cost_training.append(loss)\n",
    "        cost_validation.append(criterion(model(validation_set2.x),validation_set2.y))\n",
    "#         if epoch > 500: #this will lower the learning rate at the end of the training\n",
    "#             optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate/10)\n",
    "    if plot_epoch:\n",
    "        fig, ax1 = plt.subplots()\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('epoch)')\n",
    "        ax1.set_ylabel('cost', color=color)\n",
    "        ax1.plot(cost_training, color=color, label='training') \n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('cost', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(cost_validation, color=color, label='validation') \n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        fig.legend()\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.show()\n",
    "#     plt.figure()\n",
    "#     plt.plot(cost_training, label='training') \n",
    "#     plt.plot(cost_validation, label='validation') \n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a single item from enumerate\n",
    "\n",
    "# singleitem = next(iter(train_loader))\n",
    "# x = singleitem[0]\n",
    "# y = singleitem[1]\n",
    "# yhat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model \n",
    "cols = len(pred_vars)\n",
    "model = Net(cols, p=0.1)\n",
    "model.train()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "learning_rate = 0.01\n",
    "criterion = torch.nn.MSELoss(reduction='mean')  # this is for regression mean squared loss \n",
    "# criterion = torch.nn.L1Loss() # this is for regression mean absolute loss \n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train_loader=DataLoader(dataset=data_set,batch_size=32, shuffle=True)\n",
    "# train_mini_batches(data_set, train_loader, validation_set, model,criterion, optimizer, epochs=100, plot_epoch = True)\n",
    "train(training_set2, validation_set2, model,criterion, optimizer, learning_rate, epochs=1000, plot_epoch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "prediction_set_aggregated_df = prediction_set_aggregated_df.rename(columns={\"eonr_12\": \"eonr\"}) #needed for tha Date class\n",
    "prediction_set = scaler.transform(prediction_set_aggregated_df[pred_vars+['eonr']])\n",
    "prediction_set = np.delete(prediction_set, 20, 1)\n",
    "prediction_set = torch.FloatTensor(prediction_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(prediction_set)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with returned model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X = Variable(torch.FloatTensor(prediction_set_aggregated_df[pred_vars].values)) \n",
    "model.eval()\n",
    "y_pred = model(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred'], 'o', color='black')\n",
    "plt.xlabel('eonr_12')\n",
    "plt.ylabel('eonr_pred_cnn')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred_rf'], 'o', color='black')\n",
    "plt.xlabel('eonr_12')\n",
    "plt.ylabel('eonr_pred_rf')\n",
    "plt.show()\n",
    "\n",
    "plot_df = prediction_set_aggregated_df.groupby('eonr_12', as_index=False)['eonr_pred','eonr_pred_rf'].mean()\n",
    "plt.figure()\n",
    "plt.plot(plot_df['eonr_12'], plot_df['eonr_pred'], label='cnn') \n",
    "plt.plot(plot_df['eonr_12'], plot_df['eonr_pred_rf'], label='rf') \n",
    "plt.xlabel('eonr_12')\n",
    "plt.ylabel('eonr_pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(prediction_set_aggregated_df['eonr_pred'].min(),\n",
    "prediction_set_aggregated_df['eonr_pred'].max(),\n",
    "prediction_set_aggregated_df['eonr_pred'].mean(),\n",
    "mean_squared_error(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred']),\n",
    "mean_squared_error(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred_rf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4050 3472 (21, 21, 10, 1)  3322 (21,10,5,1) 2983 (10, 5, 5, 5, 1) 2167 2164 3004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([100,160],dtype=torch.float32)\n",
    "yhat= torch.tensor([110,170],dtype=torch.float32)\n",
    "criterion1 = torch.nn.L1Loss()\n",
    "criterion2 = torch.nn.MSELoss(reduction='mean')  # this is for regression mean squared loss \n",
    "print(criterion1(y, yhat), criterion2(y, yhat))\n",
    "criterion_up(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_up(y, yhat):\n",
    "    errors = yhat - y\n",
    "    errors_subpred = errors[errors < 0]\n",
    "    errors_overpred = errors[errors > 0]\n",
    "    if errors_subpred.data.nelement()>0: #correction for zero length\n",
    "        criterion_subpred = sum((errors_subpred)**2) / errors_subpred.data.nelement()\n",
    "    else:\n",
    "        criterion_subpred = torch.tensor([0],dtype=torch.float32)\n",
    "    if errors_overpred.data.nelement()>0: #correction for zero length\n",
    "        criterion_overpred = sum((errors_overpred)) / errors_overpred.data.nelement()\n",
    "    else:\n",
    "        criterion_overpred = torch.tensor([0],dtype=torch.float32)\n",
    "    criterion = criterion_subpred.add(criterion_overpred) \n",
    "    criterion\n",
    "    return(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat = model(data_set.x)\n",
    "y = torch.tensor([100,160,170,130],dtype=torch.float32)\n",
    "yhat= torch.tensor([90,170,150,150],dtype=torch.float32)\n",
    "\n",
    "# loss=criterion_up(yhat,data_set.y)\n",
    "errors = yhat - y\n",
    "errors\n",
    "errors_subpred = errors[errors < 0]\n",
    "errors_overpred = errors[errors > 0]\n",
    "\n",
    "if errors_subpred.data.nelement()>0: #correction for zero length\n",
    "    criterion_subpred = sum((errors_subpred)**2) / errors_subpred.data.nelement()\n",
    "else:\n",
    "    criterion_subpred = torch.tensor([0],dtype=torch.float32)\n",
    "if errors_overpred.data.nelement()>0: #correction for zero length\n",
    "    criterion_overpred = sum((errors_overpred)) / errors_overpred.data.nelement()\n",
    "else:\n",
    "    criterion_overpred = torch.tensor([0],dtype=torch.float32)\n",
    "criterion = criterion_subpred.add(criterion_overpred) \n",
    "criterion\n",
    "yhat = model(data_set.x)\n",
    "criterion = criterion_up\n",
    "criterion(data_set.y,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(TrainSet_eonr2_df, policy, pred_vars):\n",
    "    #Define training hyperprameters.\n",
    "    data_set=Data(TrainSet_eonr2_df, pred_vars)\n",
    "    \n",
    "    cols = len(pred_vars)\n",
    "\n",
    "    # Create our model \n",
    "    model = Net(cols, p=0.1)\n",
    "    model.train()\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    learning_rate = 0.01\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')  # this is for regression mean squared loss \n",
    "    # criterion = torch.nn.L1Loss() # this is for regression mean absolute loss \n",
    "\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train(data_set, validation_set, model,criterion, optimizer, learning_rate, epochs=1000, plot_epoch = False)\n",
    "\n",
    "    path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "    torch.save(model.state_dict(), path)\n",
    "    return(model)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = 'ratio_5'\n",
    "pred_vars = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/pred_vars.rds\")[None] # also works for RData\n",
    "pred_vars = [item for sublist in pred_vars.values.tolist()  for item in sublist]\n",
    "TrainSet_eonr2_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/TrainSet_eonr2.rds\")[None] # also works for RData\n",
    "net_return = build_cnn(TrainSet_eonr2_df, policy, pred_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with returned model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X = Variable(torch.FloatTensor(prediction_set_aggregated_df[pred_vars].values)) \n",
    "\n",
    "y_pred = model(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with returned model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X_pred = prediction_set_aggregated_df[pred_vars]\n",
    "\n",
    "X_pred=X_pred.values\n",
    "\n",
    "X = Variable(torch.FloatTensor(X_pred)) \n",
    "y_pred = net_return(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "y_pred\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "\n",
    "# now let's write a Rds\n",
    "pyreadr.write_rds(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_cnn_dt.rds\", prediction_set_aggregated_df)\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved model\n",
    "policy = 'ratio_5'\n",
    "path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "net_load = Net(21, 100, 1)\n",
    "net_load.load_state_dict(torch.load(path))\n",
    "net_load.eval()\n",
    "net_load.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with the saved model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X_pred = prediction_set_aggregated_df[pred_vars]\n",
    "X_pred=X_pred.values\n",
    "X_pred\n",
    "X = Variable(torch.FloatTensor(X_pred)) \n",
    "y_pred = net_load(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "y_pred\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "\n",
    "# now let's write a Rds\n",
    "pyreadr.write_rds(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_cnn_dt.rds\", prediction_set_aggregated_df)\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that loads the saved model and does predictions\n",
    "\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None]\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn(prediction_set_aggregated_df, policy, pred_vars):\n",
    "    #Load the saved model\n",
    "    #policy = 'ratio_5'\n",
    "    cols = len(pred_vars)\n",
    "    model_load = Net(cols)\n",
    "    path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "    model_load.load_state_dict(torch.load(path))\n",
    "    model_load.eval()\n",
    "    model_load.state_dict()\n",
    "    X_pred = prediction_set_aggregated_df[pred_vars]\n",
    "    X_pred=X_pred.values\n",
    "    X_pred\n",
    "    X = Variable(torch.FloatTensor(X_pred)) \n",
    "    y_pred = model_load(X) #This outputs the value for regression\n",
    "    y_pred=y_pred.data[:,0].numpy()\n",
    "    y_pred\n",
    "    prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "    return(prediction_set_aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "prediction_set_aggregated_df2 = predict_cnn(prediction_set_aggregated_df, 'ratio_5', pred_vars)\n",
    "prediction_set_aggregated_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(prediction_set_aggregated_df['eonr_pred'].min(),\n",
    "prediction_set_aggregated_df['eonr_pred'].max(),\n",
    "prediction_set_aggregated_df['eonr_pred'].mean(),\n",
    "mean_squared_error(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3088\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "x = prediction_set_aggregated_df['eonr_12']\n",
    "y = prediction_set_aggregated_df['eonr_pred']\n",
    "\n",
    "plt.scatter(x, y, c=\"g\", alpha=0.5, marker=r'$\\clubsuit$',\n",
    "            label=\"Luck\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GEOANN]",
   "language": "python",
   "name": "conda-env-GEOANN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
