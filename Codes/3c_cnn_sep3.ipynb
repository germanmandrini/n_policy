{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pyreadr\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "# check pytorch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class to get our dataset.\n",
    "class Data(Dataset):\n",
    "    def __init__(self, TrainSet_eonr2_df):\n",
    "        self.x=torch.FloatTensor(TrainSet_eonr2_df[pred_vars].values)\n",
    "        self.y=torch.FloatTensor(TrainSet_eonr2_df['eonr'].values)\n",
    "        self.y=self.y.view(-1,1)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):         \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our training dataset.\n",
    "pred_vars = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/pred_vars.rds\")[None] # also works for RData\n",
    "pred_vars = [item for sublist in pred_vars.values.tolist()  for item in sublist]\n",
    "\n",
    "TrainSet_eonr2_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/TrainSet_eonr2.rds\")[None] # also works for RData\n",
    "data_set=Data(TrainSet_eonr2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our validation dataset.\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "prediction_set_aggregated_df = prediction_set_aggregated_df.rename(columns={\"eonr_12\": \"eonr\"})\n",
    "validation_set = Data(prediction_set_aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class for model \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, cols):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(cols, 21)   # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(21, 10)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(10, 1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))      # activation function for hidden layer\n",
    "        x = F.relu(self.hidden2(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "    \n",
    "cols = len(pred_vars)\n",
    "model = Net(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (hidden1): Linear(in_features=21, out_features=10, bias=True)\n",
       "  (hidden2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the class for model (with Dropout)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, cols, p=0):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p=p)\n",
    "        self.hidden1 = torch.nn.Linear(cols, 10)   # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(10, 10)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(10, 1)   # output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop(self.hidden1(x)))\n",
    "        x = F.relu(self.drop(self.hidden2(x)))\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "cols = len(pred_vars)\n",
    "model = Net(cols, p=0.2)\n",
    "# Set the model to training mode\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class for model (simple)\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, cols):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(cols, 1)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(1, 1)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "    \n",
    "cols = len(pred_vars)\n",
    "model = Net(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one prediction\n",
    "x = data_set[0][0]\n",
    "y = data_set[0][0]\n",
    "print(model.state_dict())\n",
    "print(model(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the function to train our model, which accumulate lost for each iteration to obtain the cost.\n",
    "def train_mini_batches(data_set, train_loader, validation_set, model,criterion, optimizer, epochs=5):\n",
    "    cost_training=[] \n",
    "    cost_validation=[]\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            yhat=model(x)\n",
    "            loss=criterion(yhat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total+=loss.item()\n",
    "        cost_training.append(criterion(model(data_set.x),data_set.y))\n",
    "        cost_validation.append(criterion(model(validation_set.x),validation_set.y))\n",
    "    plt.figure()\n",
    "    plt.plot(cost_training, label='training') \n",
    "    plt.plot(cost_validation, label='validation') \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('cost')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_set, validation_set, model,criterion, optimizer, epochs=5):\n",
    "    cost_training=[] \n",
    "    cost_validation=[]  \n",
    "    for epoch in range(epochs):\n",
    "         #all the samples are used for training because they fit in memory\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(data_set.x)\n",
    "        loss=criterion(yhat,data_set.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cost_training.append(loss)\n",
    "        cost_validation.append(criterion(model(validation_set.x),validation_set.y))\n",
    "    plt.figure()\n",
    "    plt.plot(cost_training, label='training') \n",
    "    plt.plot(cost_validation, label='validation') \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('cost')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a single item from enumerate\n",
    "\n",
    "# singleitem = next(iter(train_loader))\n",
    "# x = singleitem[0]\n",
    "# y = singleitem[1]\n",
    "# yhat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(yhat,y)\n",
    "# criterion(yhat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/GEOANN/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# Create our model \n",
    "\n",
    "torch.manual_seed(0)\n",
    "learning_rate = 0.01\n",
    "criterion = torch.nn.MSELoss(size_average=False)  # this is for regression mean squared loss\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader=DataLoader(dataset=data_set,batch_size=32)\n",
    "train_mini_batches(data_set, train_loader, validation_set, model,criterion, optimizer, epochs=1000)\n",
    "# train(data_set, validation_set, model,criterion, optimizer, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one prediction after training\n",
    "model.eval()\n",
    "x = data_set[0][0]\n",
    "y = data_set[0][1]\n",
    "model.state_dict()\n",
    "print(y, model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with returned model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X = Variable(torch.FloatTensor(prediction_set_aggregated_df[pred_vars].values)) \n",
    "model.eval()\n",
    "y_pred = model(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "\n",
    "plt.plot(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred'], 'o', color='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(prediction_set_aggregated_df['eonr_pred'].min(),\n",
    "prediction_set_aggregated_df['eonr_pred'].max(),\n",
    "prediction_set_aggregated_df['eonr_pred'].mean(),\n",
    "mean_squared_error(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4050 3593 (21, 21, 10, 1) 3130 3250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(TrainSet_eonr2_df, policy, pred_vars):\n",
    "    #Define training hyperprameters.\n",
    "    batch_size = 50\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.01\n",
    "    size_hidden= 100\n",
    "    \n",
    "    y_train = TrainSet_eonr2_df['eonr']\n",
    "    #X_train = TrainSet_eonr2_df.drop('eonr', axis=1)\n",
    "    X_train = TrainSet_eonr2_df[pred_vars]\n",
    "    \n",
    "    X_train=X_train.values\n",
    "    y_train=y_train.values\n",
    "\n",
    "    #Calculate some other hyperparameters based on data.  \n",
    "    batch_no = len(X_train) // batch_size  #batches\n",
    "    cols=X_train.shape[1] #Number of columns in input matrix\n",
    "    n_output=1\n",
    "\n",
    "    #Create the model\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "    print(\"Executing the model on :\",device)\n",
    "\n",
    "    net = Net(cols, size_hidden, n_output)\n",
    "\n",
    "    #Adam is a specific flavor of gradient decent which is typically better\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "    criterion = torch.nn.MSELoss(size_average=False)  # this is for regression mean squared loss\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        #Shuffle just mixes up the dataset between epocs\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # Mini batch learning\n",
    "        for i in range(batch_no):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            inputs = Variable(torch.FloatTensor(X_train[start:end]))\n",
    "            labels = Variable(torch.FloatTensor(y_train[start:end]))\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            #print(\"outputs\",outputs)\n",
    "            #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
    "            loss = criterion(outputs, torch.unsqueeze(labels,dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        #print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "        running_loss = 0.0\n",
    "    path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "    torch.save(net.state_dict(), path)\n",
    "    return(net)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with returned model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X = Variable(torch.FloatTensor(prediction_set_aggregated_df[pred_vars].values)) \n",
    "\n",
    "y_pred = model(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with returned model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X_pred = prediction_set_aggregated_df[pred_vars]\n",
    "\n",
    "X_pred=X_pred.values\n",
    "\n",
    "X = Variable(torch.FloatTensor(X_pred)) \n",
    "y_pred = net_return(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "y_pred\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "\n",
    "# now let's write a Rds\n",
    "pyreadr.write_rds(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_cnn_dt.rds\", prediction_set_aggregated_df)\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved model\n",
    "policy = 'ratio_5'\n",
    "path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "net_load = Net(21, 100, 1)\n",
    "net_load.load_state_dict(torch.load(path))\n",
    "net_load.eval()\n",
    "net_load.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with the saved model\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "\n",
    "X_pred = prediction_set_aggregated_df[pred_vars]\n",
    "X_pred=X_pred.values\n",
    "X_pred\n",
    "X = Variable(torch.FloatTensor(X_pred)) \n",
    "y_pred = net_load(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "y_pred\n",
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "\n",
    "# now let's write a Rds\n",
    "pyreadr.write_rds(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_cnn_dt.rds\", prediction_set_aggregated_df)\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that loads the saved model and does predictions\n",
    "\n",
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None]\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn(prediction_set_aggregated_df, policy, pred_vars):\n",
    "    #Load the saved model\n",
    "    #policy = 'ratio_5'\n",
    "    path = '/home/germanm2/n_policy_box/Data/files_rds/cnn_models/'+ policy + '.pth'\n",
    "    net_load = Net(21, 100, 1)\n",
    "    net_load.load_state_dict(torch.load(path))\n",
    "    net_load.eval()\n",
    "    net_load.state_dict()\n",
    "    X_pred = prediction_set_aggregated_df[pred_vars]\n",
    "    X_pred=X_pred.values\n",
    "    X_pred\n",
    "    X = Variable(torch.FloatTensor(X_pred)) \n",
    "    y_pred = net_load(X) #This outputs the value for regression\n",
    "    y_pred=y_pred.data[:,0].numpy()\n",
    "    y_pred\n",
    "    prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "    return(prediction_set_aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "prediction_set_aggregated_df2 = predict_cnn(prediction_set_aggregated_df, 'ratio_5', pred_vars)\n",
    "prediction_set_aggregated_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(prediction_set_aggregated_df['eonr_pred'].min(),\n",
    "prediction_set_aggregated_df['eonr_pred'].max(),\n",
    "prediction_set_aggregated_df['eonr_pred'].mean(),\n",
    "mean_squared_error(prediction_set_aggregated_df['eonr_12'], prediction_set_aggregated_df['eonr_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3088\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "x = prediction_set_aggregated_df['eonr_12']\n",
    "y = prediction_set_aggregated_df['eonr_pred']\n",
    "\n",
    "plt.scatter(x, y, c=\"g\", alpha=0.5, marker=r'$\\clubsuit$',\n",
    "            label=\"Luck\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GEOANN]",
   "language": "python",
   "name": "conda-env-GEOANN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
