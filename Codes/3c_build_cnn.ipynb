{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n",
    "# https://colab.research.google.com/github/rpi-techfundamentals/website_spring_2020/blob/master/content/notebooks/20-deep-learning1/06-regression-bh-pytorch.ipynb#scrollTo=xD9PhAU7hoqT\n",
    "#!pip install torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import scipy\n",
    "#Define the model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet_eonr2_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/TrainSet_eonr2.rds\")[None] # also works for RData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = TrainSet_eonr2_df['eonr']\n",
    "X_train = TrainSet_eonr2_df.drop('eonr', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training hyperprameters.\n",
    "batch_size = 50\n",
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "size_hidden= 100\n",
    "\n",
    "#Calculate some other hyperparameters based on data.  \n",
    "batch_no = len(X_train) // batch_size  #batches\n",
    "cols=X_train.shape[1] #Number of columns in input matrix\n",
    "n_output=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the model on : cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Create the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(\"Executing the model on :\",device)\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, size_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(cols, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "net = Net(cols, size_hidden, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/GEOANN/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#Adam is a specific flavor of gradient decent which is typically better\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "criterion = torch.nn.MSELoss(size_average=False)  # this is for regression mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.values\n",
    "y_train=y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:  94476170.8125\n",
      "Epoch 2 loss:  13705082.15625\n",
      "Epoch 3 loss:  13942815.8125\n",
      "Epoch 4 loss:  13170707.78125\n",
      "Epoch 5 loss:  12666447.015625\n",
      "Epoch 6 loss:  13131437.359375\n",
      "Epoch 7 loss:  14466283.3203125\n",
      "Epoch 8 loss:  14720354.953125\n",
      "Epoch 9 loss:  12473419.765625\n",
      "Epoch 10 loss:  12144465.921875\n",
      "Epoch 11 loss:  13613768.203125\n",
      "Epoch 12 loss:  13292398.421875\n",
      "Epoch 13 loss:  12075923.05078125\n",
      "Epoch 14 loss:  12183000.796875\n",
      "Epoch 15 loss:  10990674.7109375\n",
      "Epoch 16 loss:  11348487.9296875\n",
      "Epoch 17 loss:  11049314.6875\n",
      "Epoch 18 loss:  10745119.66015625\n",
      "Epoch 19 loss:  10626338.75390625\n",
      "Epoch 20 loss:  10662782.01953125\n",
      "Epoch 21 loss:  10675387.53515625\n",
      "Epoch 22 loss:  10447506.546875\n",
      "Epoch 23 loss:  11729072.40625\n",
      "Epoch 24 loss:  10537276.5859375\n",
      "Epoch 25 loss:  10206990.55078125\n",
      "Epoch 26 loss:  10152508.328125\n",
      "Epoch 27 loss:  10304499.625\n",
      "Epoch 28 loss:  10535505.18359375\n",
      "Epoch 29 loss:  10417202.875\n",
      "Epoch 30 loss:  10394848.01171875\n",
      "Epoch 31 loss:  10028903.0078125\n",
      "Epoch 32 loss:  10422757.77734375\n",
      "Epoch 33 loss:  10258777.88671875\n",
      "Epoch 34 loss:  10001867.7734375\n",
      "Epoch 35 loss:  10145241.30078125\n",
      "Epoch 36 loss:  9850034.1640625\n",
      "Epoch 37 loss:  10006755.40234375\n",
      "Epoch 38 loss:  9851689.64453125\n",
      "Epoch 39 loss:  9908356.1796875\n",
      "Epoch 40 loss:  10051908.81640625\n",
      "Epoch 41 loss:  10242247.62109375\n",
      "Epoch 42 loss:  9663498.6796875\n",
      "Epoch 43 loss:  9871947.1640625\n",
      "Epoch 44 loss:  9829423.1640625\n",
      "Epoch 45 loss:  9711468.30078125\n",
      "Epoch 46 loss:  9786319.0234375\n",
      "Epoch 47 loss:  9746541.49609375\n",
      "Epoch 48 loss:  9690588.50390625\n",
      "Epoch 49 loss:  9569766.84765625\n",
      "Epoch 50 loss:  9549339.89453125\n",
      "Epoch 51 loss:  9917499.36328125\n",
      "Epoch 52 loss:  9958879.0078125\n",
      "Epoch 53 loss:  9766873.44921875\n",
      "Epoch 54 loss:  9517284.50390625\n",
      "Epoch 55 loss:  9390728.76171875\n",
      "Epoch 56 loss:  9814154.33203125\n",
      "Epoch 57 loss:  9973023.9921875\n",
      "Epoch 58 loss:  9447211.17578125\n",
      "Epoch 59 loss:  9806899.46875\n",
      "Epoch 60 loss:  10369052.859375\n",
      "Epoch 61 loss:  9420081.48828125\n",
      "Epoch 62 loss:  9949425.95703125\n",
      "Epoch 63 loss:  9764792.3671875\n",
      "Epoch 64 loss:  9635417.9609375\n",
      "Epoch 65 loss:  9746456.67578125\n",
      "Epoch 66 loss:  9557973.1875\n",
      "Epoch 67 loss:  10014947.98046875\n",
      "Epoch 68 loss:  9399869.4765625\n",
      "Epoch 69 loss:  9882025.203125\n",
      "Epoch 70 loss:  9673247.28125\n",
      "Epoch 71 loss:  9605735.27734375\n",
      "Epoch 72 loss:  9216629.9296875\n",
      "Epoch 73 loss:  9541547.1796875\n",
      "Epoch 74 loss:  9003122.9453125\n",
      "Epoch 75 loss:  9225673.08984375\n",
      "Epoch 76 loss:  9358010.84375\n",
      "Epoch 77 loss:  8907716.65625\n",
      "Epoch 78 loss:  9234130.90625\n",
      "Epoch 79 loss:  9007969.38671875\n",
      "Epoch 80 loss:  8996992.43359375\n",
      "Epoch 81 loss:  8921152.4140625\n",
      "Epoch 82 loss:  9200581.28125\n",
      "Epoch 83 loss:  9322989.78515625\n",
      "Epoch 84 loss:  8865470.390625\n",
      "Epoch 85 loss:  9278965.90234375\n",
      "Epoch 86 loss:  8786855.47265625\n",
      "Epoch 87 loss:  8961169.421875\n",
      "Epoch 88 loss:  9227278.78515625\n",
      "Epoch 89 loss:  8825114.3515625\n",
      "Epoch 90 loss:  9036937.83203125\n",
      "Epoch 91 loss:  8997560.5703125\n",
      "Epoch 92 loss:  9348367.39453125\n",
      "Epoch 93 loss:  8484055.83203125\n",
      "Epoch 94 loss:  8587615.27734375\n",
      "Epoch 95 loss:  8605421.671875\n",
      "Epoch 96 loss:  8764415.80859375\n",
      "Epoch 97 loss:  8646663.64453125\n",
      "Epoch 98 loss:  8733154.74609375\n",
      "Epoch 99 loss:  8484000.203125\n",
      "Epoch 100 loss:  8585222.1953125\n",
      "Epoch 101 loss:  8349957.31640625\n",
      "Epoch 102 loss:  8691174.921875\n",
      "Epoch 103 loss:  9233414.98046875\n",
      "Epoch 104 loss:  8545458.84375\n",
      "Epoch 105 loss:  8214603.16796875\n",
      "Epoch 106 loss:  8565872.6875\n",
      "Epoch 107 loss:  8096840.20703125\n",
      "Epoch 108 loss:  7945642.921875\n",
      "Epoch 109 loss:  8598786.31640625\n",
      "Epoch 110 loss:  8628784.45703125\n",
      "Epoch 111 loss:  8056366.984375\n",
      "Epoch 112 loss:  8152945.015625\n",
      "Epoch 113 loss:  8381103.17578125\n",
      "Epoch 114 loss:  7970079.859375\n",
      "Epoch 115 loss:  8072453.1171875\n",
      "Epoch 116 loss:  7888402.35546875\n",
      "Epoch 117 loss:  7972154.78515625\n",
      "Epoch 118 loss:  8081720.3359375\n",
      "Epoch 119 loss:  8518829.2109375\n",
      "Epoch 120 loss:  8992880.81640625\n",
      "Epoch 121 loss:  8274486.11328125\n",
      "Epoch 122 loss:  7965706.34765625\n",
      "Epoch 123 loss:  8097027.90625\n",
      "Epoch 124 loss:  8323532.69140625\n",
      "Epoch 125 loss:  8206441.96484375\n",
      "Epoch 126 loss:  8577513.19921875\n",
      "Epoch 127 loss:  8538717.83984375\n",
      "Epoch 128 loss:  8221893.6484375\n",
      "Epoch 129 loss:  8052938.50390625\n",
      "Epoch 130 loss:  7993173.31640625\n",
      "Epoch 131 loss:  9325125.328125\n",
      "Epoch 132 loss:  7710874.5625\n",
      "Epoch 133 loss:  8266148.65625\n",
      "Epoch 134 loss:  7899445.203125\n",
      "Epoch 135 loss:  7900476.45703125\n",
      "Epoch 136 loss:  8296045.81640625\n",
      "Epoch 137 loss:  8218879.75390625\n",
      "Epoch 138 loss:  8240881.2109375\n",
      "Epoch 139 loss:  8594452.64453125\n",
      "Epoch 140 loss:  8114976.51953125\n",
      "Epoch 141 loss:  7916935.54296875\n",
      "Epoch 142 loss:  8038963.4453125\n",
      "Epoch 143 loss:  7873036.9921875\n",
      "Epoch 144 loss:  8037780.859375\n",
      "Epoch 145 loss:  7842777.9765625\n",
      "Epoch 146 loss:  7999784.58203125\n",
      "Epoch 147 loss:  8147685.10546875\n",
      "Epoch 148 loss:  7659315.43359375\n",
      "Epoch 149 loss:  7900995.98828125\n",
      "Epoch 150 loss:  8402869.7421875\n",
      "Epoch 151 loss:  7749302.65625\n",
      "Epoch 152 loss:  7784751.58203125\n",
      "Epoch 153 loss:  7435829.5703125\n",
      "Epoch 154 loss:  7117033.69140625\n",
      "Epoch 155 loss:  7873608.79296875\n",
      "Epoch 156 loss:  7478818.015625\n",
      "Epoch 157 loss:  7375636.078125\n",
      "Epoch 158 loss:  7531021.9453125\n",
      "Epoch 159 loss:  8073887.6328125\n",
      "Epoch 160 loss:  7823794.40625\n",
      "Epoch 161 loss:  7543814.5859375\n",
      "Epoch 162 loss:  7450068.765625\n",
      "Epoch 163 loss:  7534155.5\n",
      "Epoch 164 loss:  7537140.60546875\n",
      "Epoch 165 loss:  7865868.35546875\n",
      "Epoch 166 loss:  7665704.08203125\n",
      "Epoch 167 loss:  7254730.7578125\n",
      "Epoch 168 loss:  7555792.75\n",
      "Epoch 169 loss:  7664718.640625\n",
      "Epoch 170 loss:  7754605.1953125\n",
      "Epoch 171 loss:  7681551.15625\n",
      "Epoch 172 loss:  7436414.80859375\n",
      "Epoch 173 loss:  7526506.65234375\n",
      "Epoch 174 loss:  8191593.96484375\n",
      "Epoch 175 loss:  7807532.97265625\n",
      "Epoch 176 loss:  8000021.9609375\n",
      "Epoch 177 loss:  7767873.83203125\n",
      "Epoch 178 loss:  8125733.48046875\n",
      "Epoch 179 loss:  7174067.91015625\n",
      "Epoch 180 loss:  7453572.30859375\n",
      "Epoch 181 loss:  7477269.41796875\n",
      "Epoch 182 loss:  7430678.19921875\n",
      "Epoch 183 loss:  7386504.22265625\n",
      "Epoch 184 loss:  7347861.84375\n",
      "Epoch 185 loss:  7842858.37890625\n",
      "Epoch 186 loss:  8089812.48046875\n",
      "Epoch 187 loss:  7380706.16796875\n",
      "Epoch 188 loss:  7659926.8515625\n",
      "Epoch 189 loss:  7523041.1015625\n",
      "Epoch 190 loss:  7637919.25390625\n",
      "Epoch 191 loss:  7167184.203125\n",
      "Epoch 192 loss:  8216739.234375\n",
      "Epoch 193 loss:  7215374.05078125\n",
      "Epoch 194 loss:  7455444.19921875\n",
      "Epoch 195 loss:  7229405.85546875\n",
      "Epoch 196 loss:  7779445.421875\n",
      "Epoch 197 loss:  7593593.38671875\n",
      "Epoch 198 loss:  7342974.30859375\n",
      "Epoch 199 loss:  7608137.51171875\n",
      "Epoch 200 loss:  7226258.04296875\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    #Shuffle just mixes up the dataset between epocs\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    # Mini batch learning\n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = Variable(torch.FloatTensor(X_train[start:end]))\n",
    "        labels = Variable(torch.FloatTensor(y_train[start:end]))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(\"outputs\",outputs)\n",
    "        #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
    "        loss = criterion(outputs, torch.unsqueeze(labels,dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "    running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"/home/germanm2/n_policy_box/Data/files_rds/TrainSet_eonr2.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[215.9662],\n",
       "        [125.2143],\n",
       "        [133.1097],\n",
       "        ...,\n",
       "        [192.9201],\n",
       "        [183.6675],\n",
       "        [212.6910]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "X = Variable(torch.FloatTensor(X_train)) \n",
    "result = net(X)\n",
    "result\n",
    "# pred=result.data[:,0].numpy()\n",
    "# print(len(pred),len(y_train))\n",
    "# r2_score(pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set_aggregated_df = pyreadr.read_r(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_dt.rds\")[None] # also works for RData\n",
    "prediction_set_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = prediction_set_aggregated_df[['rain_30', 'rain_60','rain_90', 't_max_30', 't_max_60', 't_max_90', 't_min_30', 't_min_60', 't_min_90', 'Y_prev',\n",
    " 'Y_corn_lt_avg', 'day_sow', 'day_v5', 'lai_v5', 'whc', 'oc_20cm_v5', 'sw_dep_v5', 'n_0_60cm_v5', 'surfaceom_wt_v5', 'sand_40cm', 'clay_40cm']]\n",
    "X_pred=X_pred.values\n",
    "X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a little bit tricky to get the resulting prediction.  \n",
    "def calculate_r2(x,y=[]):\n",
    "    \"\"\"\n",
    "    This function will return the r2 if passed x and y or return predictions if just passed x. \n",
    "    \"\"\"\n",
    "    # Evaluate the model with the test set. \n",
    "    X = Variable(torch.FloatTensor(x))  \n",
    "    result = net(X) #This outputs the value for regression\n",
    "    result=result.data[:,0].numpy()\n",
    "  \n",
    "    if len(y) != 0:\n",
    "        r2=r2_score(result, y)\n",
    "        print(\"R-Squared\", r2)\n",
    "        #print('Accuracy {:.2f}'.format(num_right / len(y)), \"for a total of \", len(y), \"records\")\n",
    "        return pd.DataFrame(data= {'actual': y, 'predicted': result})\n",
    "    else:\n",
    "        print(\"returning predictions\")\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the test set. \n",
    "X = Variable(torch.FloatTensor(X_pred)) \n",
    "y_pred = net(X) #This outputs the value for regression\n",
    "y_pred=y_pred.data[:,0].numpy()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set_aggregated_df['eonr_pred'] = y_pred\n",
    "prediction_set_aggregated_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's write a Rds\n",
    "pyreadr.write_rds(\"/home/germanm2/n_policy_box/Data/files_rds/prediction_set_aggregated_cnn_dt.rds\", prediction_set_aggregated_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GEOANN]",
   "language": "python",
   "name": "conda-env-GEOANN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
